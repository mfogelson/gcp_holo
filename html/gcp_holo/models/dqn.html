<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>gcp_holo.models.dqn API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gcp_holo.models.dqn</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import Any, Dict, List, Optional, Tuple, Type, Union

import gym
import numpy as np
import torch as th
from torch.nn import functional as F

from stable_baselines3.common.buffers import ReplayBuffer
from stable_baselines3.common.preprocessing import maybe_transpose
from stable_baselines3.common.type_aliases import GymEnv, Schedule
from stable_baselines3.common.utils import is_vectorized_observation
from stable_baselines3.common.type_aliases import GymEnv,Schedule
from stable_baselines3.common.noise import ActionNoise
from stable_baselines3 import DQN
from stable_baselines3.dqn.policies import QNetwork, DQNPolicy

from typing import Any, Dict, List, Optional, Type

import gym
import torch as th
from torch import nn

from stable_baselines3.common.torch_layers import (
    BaseFeaturesExtractor,
    FlattenExtractor,
)
from stable_baselines3.common.type_aliases import Schedule


class CustomQNetwork(QNetwork):
    &#34;&#34;&#34;
    Action-Value (Q-Value) network for DQN

    :param observation_space: Observation space
    :param action_space: Action space
    :param net_arch: The specification of the policy and value networks.
    :param activation_fn: Activation function
    :param normalize_images: Whether to normalize images or not,
         dividing by 255.0 (True by default)
    &#34;&#34;&#34;

    def __init__(
        self,
        observation_space: gym.spaces.Space,
        action_space: gym.spaces.Space,
        features_extractor: nn.Module,
        features_dim: int,
        net_arch: Optional[List[int]] = None,
        activation_fn: Type[nn.Module] = nn.ReLU,
        normalize_images: bool = True,
    ):

        super(CustomQNetwork, self).__init__(
            observation_space,
            action_space,
            features_extractor,
            features_dim, 
            net_arch, 
            activation_fn,
            normalize_images,
        )


    def forward(self, obs: th.Tensor):
        &#34;&#34;&#34;
        Predict the q-values.

        :param obs: Observation
        :return: The estimated Q-Value for each action.
        &#34;&#34;&#34;
        action_mask = obs[:, -self.action_space.n:].bool() #[&#39;action_mask&#39;].bool()

        return self.q_net(self.extract_features(obs)).masked_fill_(~action_mask, -1.0) #.softmax(1)




class CustomDQNPolicy(DQNPolicy):
    &#34;&#34;&#34;
    Policy class with Q-Value Net and target net for DQN

    :param observation_space: Observation space
    :param action_space: Action space
    :param lr_schedule: Learning rate schedule (could be constant)
    :param net_arch: The specification of the policy and value networks.
    :param activation_fn: Activation function
    :param features_extractor_class: Features extractor to use.
    :param features_extractor_kwargs: Keyword arguments
        to pass to the features extractor.
    :param normalize_images: Whether to normalize images or not,
         dividing by 255.0 (True by default)
    :param optimizer_class: The optimizer to use,
        ``th.optim.Adam`` by default
    :param optimizer_kwargs: Additional keyword arguments,
        excluding the learning rate, to pass to the optimizer
    &#34;&#34;&#34;

    def __init__(
        self,
        observation_space: gym.spaces.Space,
        action_space: gym.spaces.Space,
        lr_schedule: Schedule,
        net_arch: Optional[List[int]] = None,
        activation_fn: Type[nn.Module] = nn.ReLU,
        features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,
        features_extractor_kwargs: Optional[Dict[str, Any]] = None,
        normalize_images: bool = True,
        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,
        optimizer_kwargs: Optional[Dict[str, Any]] = None,
    ):
        super(CustomDQNPolicy, self).__init__(
            observation_space,
            action_space,
            lr_schedule, 
            net_arch, 
            activation_fn,
            features_extractor_class,
            features_extractor_kwargs,
            normalize_images,
            optimizer_class=optimizer_class,
            optimizer_kwargs=optimizer_kwargs,
        )
        


    def make_q_net(self):
        # Make sure we always have separate networks for features extractors etc

        net_args = self._update_features_extractor(self.net_args, features_extractor=None)
        return CustomQNetwork(**net_args).to(self.device)
    


class CustomDQN(DQN):
    def __init__(
        self,
        policy: Union[str, Type[DQNPolicy]],
        env: Union[GymEnv, str],
        learning_rate: Union[float, Schedule] = 1e-4,
        buffer_size: int = 1_000_000,  # 1e6
        learning_starts: int = 50000,
        batch_size: int = 32,
        tau: float = 1.0,
        gamma: float = 0.99,
        train_freq: Union[int, Tuple[int, str]] = 4,
        gradient_steps: int = 1,
        replay_buffer_class: Optional[ReplayBuffer] = None,
        replay_buffer_kwargs: Optional[Dict[str, Any]] = None,
        optimize_memory_usage: bool = False,
        target_update_interval: int = 10000,
        exploration_fraction: float = 0.1,
        exploration_initial_eps: float = 1.0,
        exploration_final_eps: float = 0.05,
        max_grad_norm: float = 10,
        tensorboard_log: Optional[str] = None,
        create_eval_env: bool = False,
        policy_kwargs: Optional[Dict[str, Any]] = None,
        verbose: int = 0,
        seed: Optional[int] = None,
        device: Union[th.device, str] = &#34;auto&#34;,
        _init_setup_model: bool = True,
    ):

        super(CustomDQN, self).__init__(
            policy,
            env,
            learning_rate,
            buffer_size,
            learning_starts,
            batch_size,
            tau,
            gamma, 
            train_freq, 
            gradient_steps, 
            replay_buffer_class, 
            replay_buffer_kwargs, 
            optimize_memory_usage, 
            target_update_interval, 
            exploration_fraction, 
            exploration_initial_eps, 
            exploration_final_eps, 
            max_grad_norm, 
            tensorboard_log, 
            create_eval_env, 
            policy_kwargs, 
            verbose, 
            seed, 
            device, 
            _init_setup_model
        )
        
    def _sample_action(
        self,
        learning_starts: int,
        action_noise: Optional[ActionNoise] = None,
        n_envs: int = 1,
    ):
        &#34;&#34;&#34;
        Sample an action according to the exploration policy.
        This is either done by sampling the probability distribution of the policy,
        or sampling a random action (from a uniform distribution over the action space)
        or by adding noise to the deterministic output.

        :param action_noise: Action noise that will be used for exploration
            Required for deterministic policy (e.g. TD3). This can also be used
            in addition to the stochastic policy for SAC.
        :param learning_starts: Number of steps before learning for the warm-up phase.
        :param n_envs:
        :return: action to take in the environment
            and scaled action that will be stored in the replay buffer.
            The two differs when the action space is not normalized (bounds are not [-1, 1]).
        &#34;&#34;&#34;
        # Select action randomly or according to policy
        if self.num_timesteps &lt; learning_starts and not (self.use_sde and self.use_sde_at_warmup):
            all_actions = np.arange(self.action_space.n)

            unscaled_action = np.array([np.random.choice(all_actions, 1, p=self._last_obs[i, -self.action_space.n:]/self._last_obs[i, -self.action_space.n:].sum()) for i in range(n_envs)])
            
        else:
            # Note: when using continuous actions,
            # we assume that the policy uses tanh to scale the action
            # We use non-deterministic action in the case of SAC, for TD3, it does not matter
            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)

        # Rescale the action from [low, high] to [-1, 1]
        if isinstance(self.action_space, gym.spaces.Box):
            scaled_action = self.policy.scale_action(unscaled_action)

            # Add noise to the action (improve exploration)
            if action_noise is not None:
                scaled_action = np.clip(scaled_action + action_noise(), -1, 1)

            # We store the scaled action in the buffer
            buffer_action = scaled_action
            action = self.policy.unscale_action(scaled_action)
        else:
            # Discrete case, no need to normalize or clip
            buffer_action = unscaled_action
            action = buffer_action
            
        return action, buffer_action
    
    def train(self, gradient_steps: int, batch_size: int = 100):
        # Switch to train mode (this affects batch norm / dropout)
        self.policy.set_training_mode(True)
        # Update learning rate according to schedule
        self._update_learning_rate(self.policy.optimizer)

        losses = []
        for _ in range(gradient_steps):
            # Sample replay buffer
            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)

            with th.no_grad():
                # Compute the next Q-values using the target network
                next_q_values = self.q_net_target(replay_data.next_observations)
                # Follow greedy policy: use the one with the highest value
                next_q_values, _ = next_q_values.max(dim=1)
                # Avoid potential broadcast issue
                next_q_values = next_q_values.reshape(-1, 1)
                # 1-step TD target
                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values
            # import pdb
            # pdb.set_trace()
            # Get current Q-values estimates
            current_q_values = self.q_net(replay_data.observations)

            # Retrieve the q-values for the actions from the replay buffer
            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())

            # Compute Huber loss (less sensitive to outliers)
            loss = F.smooth_l1_loss(current_q_values, target_q_values)
            if np.isnan(loss.detach().cpu().numpy()).any():
                import pdb
                pdb.set_trace()
                raise ValueError
                
            losses.append(loss.item())

            # Optimize the policy
            self.policy.optimizer.zero_grad()
            loss.backward()
            # Clip gradient norm
            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
            self.policy.optimizer.step()

        # Increase update counter
        self._n_updates += gradient_steps

        self.logger.record(&#34;train/n_updates&#34;, self._n_updates, exclude=&#34;tensorboard&#34;)
        self.logger.record(&#34;train/loss&#34;, np.mean(losses))


    def predict(
        self,
        observation: np.ndarray,
        state: Optional[Tuple[np.ndarray, ...]] = None,
        episode_start: Optional[np.ndarray] = None,
        deterministic: bool = False,
    ):

        &#34;&#34;&#34;
        Overrides the base_class predict function to include epsilon-greedy exploration.

        :param observation: the input observation
        :param state: The last states (can be None, used in recurrent policies)
        :param episode_start: The last masks (can be None, used in recurrent policies)
        :param deterministic: Whether or not to return deterministic actions.
        :return: the model&#39;s action and the next state
            (used in recurrent policies)
        &#34;&#34;&#34;
        if not deterministic and np.random.rand() &lt; self.exploration_rate:
            if is_vectorized_observation(maybe_transpose(observation, self.observation_space), self.observation_space):
                if isinstance(self.observation_space, gym.spaces.Dict):
                    n_batch = observation[list(observation.keys())[0]].shape[0]
                else:
                    n_batch = observation.shape[0]
         
                all_actions = np.arange(self.action_space.n)
                action = np.array([np.random.choice(all_actions, 1, p=observation[i, -self.action_space.n:]/observation[i, -self.action_space.n:].sum()) for i in range(n_batch)])
                
            else:
                action = np.array(self.action_space.sample())

        else:
            action, state = self.policy.predict(observation, state, episode_start, deterministic)
            
        return action, state</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="gcp_holo.models.dqn.CustomDQN"><code class="flex name class">
<span>class <span class="ident">CustomDQN</span></span>
<span>(</span><span>policy: Union[str, Type[stable_baselines3.dqn.policies.DQNPolicy]], env: Union[gym.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, str], learning_rate: Union[float, Callable[[float], float]] = 0.0001, buffer_size: int = 1000000, learning_starts: int = 50000, batch_size: int = 32, tau: float = 1.0, gamma: float = 0.99, train_freq: Union[int, Tuple[int, str]] = 4, gradient_steps: int = 1, replay_buffer_class: Optional[stable_baselines3.common.buffers.ReplayBuffer] = None, replay_buffer_kwargs: Optional[Dict[str, Any]] = None, optimize_memory_usage: bool = False, target_update_interval: int = 10000, exploration_fraction: float = 0.1, exploration_initial_eps: float = 1.0, exploration_final_eps: float = 0.05, max_grad_norm: float = 10, tensorboard_log: Optional[str] = None, create_eval_env: bool = False, policy_kwargs: Optional[Dict[str, Any]] = None, verbose: int = 0, seed: Optional[int] = None, device: Union[torch.device, str] = 'auto')</span>
</code></dt>
<dd>
<div class="desc"><p>Deep Q-Network (DQN)</p>
<p>Paper: <a href="https://arxiv.org/abs/1312.5602,">https://arxiv.org/abs/1312.5602,</a> <a href="https://www.nature.com/articles/nature14236">https://www.nature.com/articles/nature14236</a>
Default hyperparameters are taken from the nature paper,
except for the optimizer and learning rate that were taken from Stable Baselines defaults.</p>
<p>:param policy: The policy model to use (MlpPolicy, CnnPolicy, &hellip;)
:param env: The environment to learn from (if registered in Gym, can be str)
:param learning_rate: The learning rate, it can be a function
of the current progress remaining (from 1 to 0)
:param buffer_size: size of the replay buffer
:param learning_starts: how many steps of the model to collect transitions for before learning starts
:param batch_size: Minibatch size for each gradient update
:param tau: the soft update coefficient ("Polyak update", between 0 and 1) default 1 for hard update
:param gamma: the discount factor
:param train_freq: Update the model every <code>train_freq</code> steps. Alternatively pass a tuple of frequency and unit
like <code>(5, "step")</code> or <code>(2, "episode")</code>.
:param gradient_steps: How many gradient steps to do after each rollout (see <code>train_freq</code>)
Set to <code>-1</code> means to do as many gradient steps as steps done in the environment
during the rollout.
:param replay_buffer_class: Replay buffer class to use (for instance <code>HerReplayBuffer</code>).
If <code>None</code>, it will be automatically selected.
:param replay_buffer_kwargs: Keyword arguments to pass to the replay buffer on creation.
:param optimize_memory_usage: Enable a memory efficient variant of the replay buffer
at a cost of more complexity.
See <a href="https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195">https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195</a>
:param target_update_interval: update the target network every <code>target_update_interval</code>
environment steps.
:param exploration_fraction: fraction of entire training period over which the exploration rate is reduced
:param exploration_initial_eps: initial value of random action probability
:param exploration_final_eps: final value of random action probability
:param max_grad_norm: The maximum value for the gradient clipping
:param tensorboard_log: the log location for tensorboard (if None, no logging)
:param create_eval_env: Whether to create a second environment that will be
used for evaluating the agent periodically. (Only available when passing string for the environment)
:param policy_kwargs: additional arguments to be passed to the policy on creation
:param verbose: the verbosity level: 0 no output, 1 info, 2 debug
:param seed: Seed for the pseudo random generators
:param device: Device (cpu, cuda, &hellip;) on which the code should be run.
Setting it to auto, the code will be run on the GPU if possible.
:param _init_setup_model: Whether or not to build the network at the creation of the instance</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CustomDQN(DQN):
    def __init__(
        self,
        policy: Union[str, Type[DQNPolicy]],
        env: Union[GymEnv, str],
        learning_rate: Union[float, Schedule] = 1e-4,
        buffer_size: int = 1_000_000,  # 1e6
        learning_starts: int = 50000,
        batch_size: int = 32,
        tau: float = 1.0,
        gamma: float = 0.99,
        train_freq: Union[int, Tuple[int, str]] = 4,
        gradient_steps: int = 1,
        replay_buffer_class: Optional[ReplayBuffer] = None,
        replay_buffer_kwargs: Optional[Dict[str, Any]] = None,
        optimize_memory_usage: bool = False,
        target_update_interval: int = 10000,
        exploration_fraction: float = 0.1,
        exploration_initial_eps: float = 1.0,
        exploration_final_eps: float = 0.05,
        max_grad_norm: float = 10,
        tensorboard_log: Optional[str] = None,
        create_eval_env: bool = False,
        policy_kwargs: Optional[Dict[str, Any]] = None,
        verbose: int = 0,
        seed: Optional[int] = None,
        device: Union[th.device, str] = &#34;auto&#34;,
        _init_setup_model: bool = True,
    ):

        super(CustomDQN, self).__init__(
            policy,
            env,
            learning_rate,
            buffer_size,
            learning_starts,
            batch_size,
            tau,
            gamma, 
            train_freq, 
            gradient_steps, 
            replay_buffer_class, 
            replay_buffer_kwargs, 
            optimize_memory_usage, 
            target_update_interval, 
            exploration_fraction, 
            exploration_initial_eps, 
            exploration_final_eps, 
            max_grad_norm, 
            tensorboard_log, 
            create_eval_env, 
            policy_kwargs, 
            verbose, 
            seed, 
            device, 
            _init_setup_model
        )
        
    def _sample_action(
        self,
        learning_starts: int,
        action_noise: Optional[ActionNoise] = None,
        n_envs: int = 1,
    ):
        &#34;&#34;&#34;
        Sample an action according to the exploration policy.
        This is either done by sampling the probability distribution of the policy,
        or sampling a random action (from a uniform distribution over the action space)
        or by adding noise to the deterministic output.

        :param action_noise: Action noise that will be used for exploration
            Required for deterministic policy (e.g. TD3). This can also be used
            in addition to the stochastic policy for SAC.
        :param learning_starts: Number of steps before learning for the warm-up phase.
        :param n_envs:
        :return: action to take in the environment
            and scaled action that will be stored in the replay buffer.
            The two differs when the action space is not normalized (bounds are not [-1, 1]).
        &#34;&#34;&#34;
        # Select action randomly or according to policy
        if self.num_timesteps &lt; learning_starts and not (self.use_sde and self.use_sde_at_warmup):
            all_actions = np.arange(self.action_space.n)

            unscaled_action = np.array([np.random.choice(all_actions, 1, p=self._last_obs[i, -self.action_space.n:]/self._last_obs[i, -self.action_space.n:].sum()) for i in range(n_envs)])
            
        else:
            # Note: when using continuous actions,
            # we assume that the policy uses tanh to scale the action
            # We use non-deterministic action in the case of SAC, for TD3, it does not matter
            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)

        # Rescale the action from [low, high] to [-1, 1]
        if isinstance(self.action_space, gym.spaces.Box):
            scaled_action = self.policy.scale_action(unscaled_action)

            # Add noise to the action (improve exploration)
            if action_noise is not None:
                scaled_action = np.clip(scaled_action + action_noise(), -1, 1)

            # We store the scaled action in the buffer
            buffer_action = scaled_action
            action = self.policy.unscale_action(scaled_action)
        else:
            # Discrete case, no need to normalize or clip
            buffer_action = unscaled_action
            action = buffer_action
            
        return action, buffer_action
    
    def train(self, gradient_steps: int, batch_size: int = 100):
        # Switch to train mode (this affects batch norm / dropout)
        self.policy.set_training_mode(True)
        # Update learning rate according to schedule
        self._update_learning_rate(self.policy.optimizer)

        losses = []
        for _ in range(gradient_steps):
            # Sample replay buffer
            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)

            with th.no_grad():
                # Compute the next Q-values using the target network
                next_q_values = self.q_net_target(replay_data.next_observations)
                # Follow greedy policy: use the one with the highest value
                next_q_values, _ = next_q_values.max(dim=1)
                # Avoid potential broadcast issue
                next_q_values = next_q_values.reshape(-1, 1)
                # 1-step TD target
                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values
            # import pdb
            # pdb.set_trace()
            # Get current Q-values estimates
            current_q_values = self.q_net(replay_data.observations)

            # Retrieve the q-values for the actions from the replay buffer
            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())

            # Compute Huber loss (less sensitive to outliers)
            loss = F.smooth_l1_loss(current_q_values, target_q_values)
            if np.isnan(loss.detach().cpu().numpy()).any():
                import pdb
                pdb.set_trace()
                raise ValueError
                
            losses.append(loss.item())

            # Optimize the policy
            self.policy.optimizer.zero_grad()
            loss.backward()
            # Clip gradient norm
            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
            self.policy.optimizer.step()

        # Increase update counter
        self._n_updates += gradient_steps

        self.logger.record(&#34;train/n_updates&#34;, self._n_updates, exclude=&#34;tensorboard&#34;)
        self.logger.record(&#34;train/loss&#34;, np.mean(losses))


    def predict(
        self,
        observation: np.ndarray,
        state: Optional[Tuple[np.ndarray, ...]] = None,
        episode_start: Optional[np.ndarray] = None,
        deterministic: bool = False,
    ):

        &#34;&#34;&#34;
        Overrides the base_class predict function to include epsilon-greedy exploration.

        :param observation: the input observation
        :param state: The last states (can be None, used in recurrent policies)
        :param episode_start: The last masks (can be None, used in recurrent policies)
        :param deterministic: Whether or not to return deterministic actions.
        :return: the model&#39;s action and the next state
            (used in recurrent policies)
        &#34;&#34;&#34;
        if not deterministic and np.random.rand() &lt; self.exploration_rate:
            if is_vectorized_observation(maybe_transpose(observation, self.observation_space), self.observation_space):
                if isinstance(self.observation_space, gym.spaces.Dict):
                    n_batch = observation[list(observation.keys())[0]].shape[0]
                else:
                    n_batch = observation.shape[0]
         
                all_actions = np.arange(self.action_space.n)
                action = np.array([np.random.choice(all_actions, 1, p=observation[i, -self.action_space.n:]/observation[i, -self.action_space.n:].sum()) for i in range(n_batch)])
                
            else:
                action = np.array(self.action_space.sample())

        else:
            action, state = self.policy.predict(observation, state, episode_start, deterministic)
            
        return action, state</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>stable_baselines3.dqn.dqn.DQN</li>
<li>stable_baselines3.common.off_policy_algorithm.OffPolicyAlgorithm</li>
<li>stable_baselines3.common.base_class.BaseAlgorithm</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="gcp_holo.models.dqn.CustomDQN.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, observation: numpy.ndarray, state: Optional[Tuple[numpy.ndarray, ...]] = None, episode_start: Optional[numpy.ndarray] = None, deterministic: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Overrides the base_class predict function to include epsilon-greedy exploration.</p>
<p>:param observation: the input observation
:param state: The last states (can be None, used in recurrent policies)
:param episode_start: The last masks (can be None, used in recurrent policies)
:param deterministic: Whether or not to return deterministic actions.
:return: the model's action and the next state
(used in recurrent policies)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(
    self,
    observation: np.ndarray,
    state: Optional[Tuple[np.ndarray, ...]] = None,
    episode_start: Optional[np.ndarray] = None,
    deterministic: bool = False,
):

    &#34;&#34;&#34;
    Overrides the base_class predict function to include epsilon-greedy exploration.

    :param observation: the input observation
    :param state: The last states (can be None, used in recurrent policies)
    :param episode_start: The last masks (can be None, used in recurrent policies)
    :param deterministic: Whether or not to return deterministic actions.
    :return: the model&#39;s action and the next state
        (used in recurrent policies)
    &#34;&#34;&#34;
    if not deterministic and np.random.rand() &lt; self.exploration_rate:
        if is_vectorized_observation(maybe_transpose(observation, self.observation_space), self.observation_space):
            if isinstance(self.observation_space, gym.spaces.Dict):
                n_batch = observation[list(observation.keys())[0]].shape[0]
            else:
                n_batch = observation.shape[0]
     
            all_actions = np.arange(self.action_space.n)
            action = np.array([np.random.choice(all_actions, 1, p=observation[i, -self.action_space.n:]/observation[i, -self.action_space.n:].sum()) for i in range(n_batch)])
            
        else:
            action = np.array(self.action_space.sample())

    else:
        action, state = self.policy.predict(observation, state, episode_start, deterministic)
        
    return action, state</code></pre>
</details>
</dd>
<dt id="gcp_holo.models.dqn.CustomDQN.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, gradient_steps: int, batch_size: int = 100)</span>
</code></dt>
<dd>
<div class="desc"><p>Sample the replay buffer and do the updates
(gradient descent and update target networks)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, gradient_steps: int, batch_size: int = 100):
    # Switch to train mode (this affects batch norm / dropout)
    self.policy.set_training_mode(True)
    # Update learning rate according to schedule
    self._update_learning_rate(self.policy.optimizer)

    losses = []
    for _ in range(gradient_steps):
        # Sample replay buffer
        replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)

        with th.no_grad():
            # Compute the next Q-values using the target network
            next_q_values = self.q_net_target(replay_data.next_observations)
            # Follow greedy policy: use the one with the highest value
            next_q_values, _ = next_q_values.max(dim=1)
            # Avoid potential broadcast issue
            next_q_values = next_q_values.reshape(-1, 1)
            # 1-step TD target
            target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values
        # import pdb
        # pdb.set_trace()
        # Get current Q-values estimates
        current_q_values = self.q_net(replay_data.observations)

        # Retrieve the q-values for the actions from the replay buffer
        current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())

        # Compute Huber loss (less sensitive to outliers)
        loss = F.smooth_l1_loss(current_q_values, target_q_values)
        if np.isnan(loss.detach().cpu().numpy()).any():
            import pdb
            pdb.set_trace()
            raise ValueError
            
        losses.append(loss.item())

        # Optimize the policy
        self.policy.optimizer.zero_grad()
        loss.backward()
        # Clip gradient norm
        th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
        self.policy.optimizer.step()

    # Increase update counter
    self._n_updates += gradient_steps

    self.logger.record(&#34;train/n_updates&#34;, self._n_updates, exclude=&#34;tensorboard&#34;)
    self.logger.record(&#34;train/loss&#34;, np.mean(losses))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="gcp_holo.models.dqn.CustomDQNPolicy"><code class="flex name class">
<span>class <span class="ident">CustomDQNPolicy</span></span>
<span>(</span><span>observation_space: gym.spaces.space.Space, action_space: gym.spaces.space.Space, lr_schedule: Callable[[float], float], net_arch: Optional[List[int]] = None, activation_fn: Type[torch.nn.modules.module.Module] = torch.nn.modules.activation.ReLU, features_extractor_class: Type[stable_baselines3.common.torch_layers.BaseFeaturesExtractor] = stable_baselines3.common.torch_layers.FlattenExtractor, features_extractor_kwargs: Optional[Dict[str, Any]] = None, normalize_images: bool = True, optimizer_class: Type[torch.optim.optimizer.Optimizer] = torch.optim.adam.Adam, optimizer_kwargs: Optional[Dict[str, Any]] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Policy class with Q-Value Net and target net for DQN</p>
<p>:param observation_space: Observation space
:param action_space: Action space
:param lr_schedule: Learning rate schedule (could be constant)
:param net_arch: The specification of the policy and value networks.
:param activation_fn: Activation function
:param features_extractor_class: Features extractor to use.
:param features_extractor_kwargs: Keyword arguments
to pass to the features extractor.
:param normalize_images: Whether to normalize images or not,
dividing by 255.0 (True by default)
:param optimizer_class: The optimizer to use,
<code>th.optim.Adam</code> by default
:param optimizer_kwargs: Additional keyword arguments,
excluding the learning rate, to pass to the optimizer</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CustomDQNPolicy(DQNPolicy):
    &#34;&#34;&#34;
    Policy class with Q-Value Net and target net for DQN

    :param observation_space: Observation space
    :param action_space: Action space
    :param lr_schedule: Learning rate schedule (could be constant)
    :param net_arch: The specification of the policy and value networks.
    :param activation_fn: Activation function
    :param features_extractor_class: Features extractor to use.
    :param features_extractor_kwargs: Keyword arguments
        to pass to the features extractor.
    :param normalize_images: Whether to normalize images or not,
         dividing by 255.0 (True by default)
    :param optimizer_class: The optimizer to use,
        ``th.optim.Adam`` by default
    :param optimizer_kwargs: Additional keyword arguments,
        excluding the learning rate, to pass to the optimizer
    &#34;&#34;&#34;

    def __init__(
        self,
        observation_space: gym.spaces.Space,
        action_space: gym.spaces.Space,
        lr_schedule: Schedule,
        net_arch: Optional[List[int]] = None,
        activation_fn: Type[nn.Module] = nn.ReLU,
        features_extractor_class: Type[BaseFeaturesExtractor] = FlattenExtractor,
        features_extractor_kwargs: Optional[Dict[str, Any]] = None,
        normalize_images: bool = True,
        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,
        optimizer_kwargs: Optional[Dict[str, Any]] = None,
    ):
        super(CustomDQNPolicy, self).__init__(
            observation_space,
            action_space,
            lr_schedule, 
            net_arch, 
            activation_fn,
            features_extractor_class,
            features_extractor_kwargs,
            normalize_images,
            optimizer_class=optimizer_class,
            optimizer_kwargs=optimizer_kwargs,
        )
        


    def make_q_net(self):
        # Make sure we always have separate networks for features extractors etc

        net_args = self._update_features_extractor(self.net_args, features_extractor=None)
        return CustomQNetwork(**net_args).to(self.device)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>stable_baselines3.dqn.policies.DQNPolicy</li>
<li>stable_baselines3.common.policies.BasePolicy</li>
<li>stable_baselines3.common.policies.BaseModel</li>
<li>torch.nn.modules.module.Module</li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="gcp_holo.models.dqn.CustomDQNPolicy.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="gcp_holo.models.dqn.CustomDQNPolicy.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="gcp_holo.models.dqn.CustomDQNPolicy.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, obs: torch.Tensor, deterministic: bool = True) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, obs: th.Tensor, deterministic: bool = True) -&gt; th.Tensor:
    return self._predict(obs, deterministic=deterministic)</code></pre>
</details>
</dd>
<dt id="gcp_holo.models.dqn.CustomDQNPolicy.make_q_net"><code class="name flex">
<span>def <span class="ident">make_q_net</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_q_net(self):
    # Make sure we always have separate networks for features extractors etc

    net_args = self._update_features_extractor(self.net_args, features_extractor=None)
    return CustomQNetwork(**net_args).to(self.device)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="gcp_holo.models.dqn.CustomQNetwork"><code class="flex name class">
<span>class <span class="ident">CustomQNetwork</span></span>
<span>(</span><span>observation_space: gym.spaces.space.Space, action_space: gym.spaces.space.Space, features_extractor: torch.nn.modules.module.Module, features_dim: int, net_arch: Optional[List[int]] = None, activation_fn: Type[torch.nn.modules.module.Module] = torch.nn.modules.activation.ReLU, normalize_images: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Action-Value (Q-Value) network for DQN</p>
<p>:param observation_space: Observation space
:param action_space: Action space
:param net_arch: The specification of the policy and value networks.
:param activation_fn: Activation function
:param normalize_images: Whether to normalize images or not,
dividing by 255.0 (True by default)</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CustomQNetwork(QNetwork):
    &#34;&#34;&#34;
    Action-Value (Q-Value) network for DQN

    :param observation_space: Observation space
    :param action_space: Action space
    :param net_arch: The specification of the policy and value networks.
    :param activation_fn: Activation function
    :param normalize_images: Whether to normalize images or not,
         dividing by 255.0 (True by default)
    &#34;&#34;&#34;

    def __init__(
        self,
        observation_space: gym.spaces.Space,
        action_space: gym.spaces.Space,
        features_extractor: nn.Module,
        features_dim: int,
        net_arch: Optional[List[int]] = None,
        activation_fn: Type[nn.Module] = nn.ReLU,
        normalize_images: bool = True,
    ):

        super(CustomQNetwork, self).__init__(
            observation_space,
            action_space,
            features_extractor,
            features_dim, 
            net_arch, 
            activation_fn,
            normalize_images,
        )


    def forward(self, obs: th.Tensor):
        &#34;&#34;&#34;
        Predict the q-values.

        :param obs: Observation
        :return: The estimated Q-Value for each action.
        &#34;&#34;&#34;
        action_mask = obs[:, -self.action_space.n:].bool() #[&#39;action_mask&#39;].bool()

        return self.q_net(self.extract_features(obs)).masked_fill_(~action_mask, -1.0) #.softmax(1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>stable_baselines3.dqn.policies.QNetwork</li>
<li>stable_baselines3.common.policies.BasePolicy</li>
<li>stable_baselines3.common.policies.BaseModel</li>
<li>torch.nn.modules.module.Module</li>
<li>abc.ABC</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="gcp_holo.models.dqn.CustomQNetwork.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="gcp_holo.models.dqn.CustomQNetwork.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="gcp_holo.models.dqn.CustomQNetwork.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, obs: torch.Tensor) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Predict the q-values.</p>
<p>:param obs: Observation
:return: The estimated Q-Value for each action.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, obs: th.Tensor):
    &#34;&#34;&#34;
    Predict the q-values.

    :param obs: Observation
    :return: The estimated Q-Value for each action.
    &#34;&#34;&#34;
    action_mask = obs[:, -self.action_space.n:].bool() #[&#39;action_mask&#39;].bool()

    return self.q_net(self.extract_features(obs)).masked_fill_(~action_mask, -1.0) #.softmax(1)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gcp_holo.models" href="index.html">gcp_holo.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="gcp_holo.models.dqn.CustomDQN" href="#gcp_holo.models.dqn.CustomDQN">CustomDQN</a></code></h4>
<ul class="">
<li><code><a title="gcp_holo.models.dqn.CustomDQN.predict" href="#gcp_holo.models.dqn.CustomDQN.predict">predict</a></code></li>
<li><code><a title="gcp_holo.models.dqn.CustomDQN.train" href="#gcp_holo.models.dqn.CustomDQN.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="gcp_holo.models.dqn.CustomDQNPolicy" href="#gcp_holo.models.dqn.CustomDQNPolicy">CustomDQNPolicy</a></code></h4>
<ul class="">
<li><code><a title="gcp_holo.models.dqn.CustomDQNPolicy.dump_patches" href="#gcp_holo.models.dqn.CustomDQNPolicy.dump_patches">dump_patches</a></code></li>
<li><code><a title="gcp_holo.models.dqn.CustomDQNPolicy.forward" href="#gcp_holo.models.dqn.CustomDQNPolicy.forward">forward</a></code></li>
<li><code><a title="gcp_holo.models.dqn.CustomDQNPolicy.make_q_net" href="#gcp_holo.models.dqn.CustomDQNPolicy.make_q_net">make_q_net</a></code></li>
<li><code><a title="gcp_holo.models.dqn.CustomDQNPolicy.training" href="#gcp_holo.models.dqn.CustomDQNPolicy.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="gcp_holo.models.dqn.CustomQNetwork" href="#gcp_holo.models.dqn.CustomQNetwork">CustomQNetwork</a></code></h4>
<ul class="">
<li><code><a title="gcp_holo.models.dqn.CustomQNetwork.dump_patches" href="#gcp_holo.models.dqn.CustomQNetwork.dump_patches">dump_patches</a></code></li>
<li><code><a title="gcp_holo.models.dqn.CustomQNetwork.forward" href="#gcp_holo.models.dqn.CustomQNetwork.forward">forward</a></code></li>
<li><code><a title="gcp_holo.models.dqn.CustomQNetwork.training" href="#gcp_holo.models.dqn.CustomQNetwork.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>