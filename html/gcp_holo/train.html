<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>gcp_holo.train API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gcp_holo.train</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import argparse
import multiprocessing
import os
import pdb
import pickle


import matplotlib.pyplot as plt
import numpy as np
from timebudget import timebudget
from itertools import repeat

import wandb

from linkage_gym.envs.Mech import Mech
from linkage_gym.utils.env_utils import normalize_curve, uniquify
from utils.utils import linear_schedule

from models.a2c import CustomActorCriticPolicy
from models.dqn import CustomDQN, CustomDQNPolicy
from models.gcpn import GNN
from models.random_search import random_search
from utils.utils import evaluate_policy
# from stable_baselines3.common.evaluation import evaluate_policy


from stable_baselines3 import A2C, PPO  # , HER, PPO1, PPO2, TRPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback

from datetime import datetime

import warnings
# warnings.simplefilter(&#34;ignore&#34;)
# def fxn():
#     warnings.warn(&#34;invalid&#34;, RuntimeWarning)

@timebudget
def main(args):
    # pdb.set_trace()
    # args.body_constraints = [-3., 3., 0., 3.]
    # args.coupler_constraints = [-3., 3., -3., 0.]
    now = datetime.now().strftime(&#34;%m_%d_%Y_%HD:%M:%S&#34;)
    day = datetime.now().strftime(&#34;%m_%d_%Y&#34;) 
    
    ## Use WANDB for logging
    os.environ[&#34;WANDB_MODE&#34;] = args.wandb_mode

    ## Learn model N times
    for trial in range(args.num_trials):
        ## Initialize WANBD for Logging
        run = wandb.init(project=args.wandb_project, 
               entity=&#39;mfogelson&#39;,
               sync_tensorboard=True,
               )
        
        ## Adds all of the arguments as config variables
        wandb.config.update(args) 

        ## Log / eval / save location
        tb_log_dir = f&#34;./logs/{args.goal_filename}/{args.model}/{day}/{run.id}&#34;
        eval_dir = f&#34;./evaluations/{args.goal_filename}/{args.model}/{day}/&#34;
        save_dir = f&#34;./trained/{args.goal_filename}/{args.model}/{day}/&#34;
        design_dir = f&#34;./designs/{args.goal_filename}/{args.model}/{day}/&#34;
        
        if not os.path.isdir(tb_log_dir):
            os.makedirs(tb_log_dir, exist_ok=True)
            
        if not os.path.isdir(eval_dir):
            os.makedirs(eval_dir, exist_ok=True)
            
        if not os.path.isdir(save_dir):
            os.makedirs(save_dir, exist_ok=True)
            
        if not os.path.isdir(design_dir):
            os.makedirs(design_dir, exist_ok=True)

        ## Load Goal information
        goal_curve = pickle.load(open(f&#39;{args.goal_path}/{args.goal_filename}.pkl&#39;, &#39;rb&#39;)) # NOTE: sometimes ordering needs to be reversed add [:,::-1]
            
        idx = np.round(np.linspace(0, goal_curve.shape[1] - 1, args.sample_points)).astype(int)
        goal = normalize_curve(goal_curve[:,idx]) #R@normalize_curve(goal_curve[:,::-1][:,idx])
        goal[:, -1] = goal[:, 0]
        
        ## Initialize Gym ENV
        env_kwargs = {&#34;max_nodes&#34;:args.max_nodes, 
                        &#34;bound&#34;:args.bound, 
                        &#34;resolution&#34;:args.resolution, 
                        &#34;sample_points&#34;:args.sample_points,
                        &#34;feature_points&#34;: args.feature_points, 
                        &#34;goal&#34;:goal, 
                        &#34;normalize&#34;:args.normalize, 
                        # &#34;seed&#34;: args.seed+trial, 
                        &#34;fixed_initial_state&#34;: args.fixed_initial_state, 
                        &#34;ordered_distance&#34;: args.ordered, 
                        &#34;constraints&#34;: [], #[args.body_constraints, args.coupler_constraints], 
                        &#34;self_loops&#34;: args.use_self_loops, 
                        &#34;use_node_type&#34;: args.use_node_type,}
        
        # If PPO A2C or DQN can use multiple envs while training
        if args.model in [&#39;PPO&#39;, &#39;A2C&#39;, &#39;DQN&#39;]:
            env = make_vec_env(Mech, n_envs=args.n_envs, env_kwargs=env_kwargs, seed=args.seed) # NOTE: For faster training use SubProcVecEnv 
        else:
            env = []
            for i in range(args.n_envs):
                env_kwargs[&#39;seed&#39;] = i+args.seed
                env.append(Mech(**env_kwargs))
                        
        ## GNN Args
        gnn_kwargs = {
                &#34;max_nodes&#34;: args.max_nodes,
                &#34;num_features&#34;: 2*args.feature_points+int(args.use_node_type),
                &#34;hidden_channels&#34;:64, 
                &#34;out_channels&#34;:64, 
                &#34;normalize&#34;:False, 
                &#34;batch_normalization&#34;:args.batch_normalize, 
                &#34;lin&#34;:True, 
                &#34;add_loop&#34;:False}
        
        ## Policy Architecture
        dqn_arch = [64, 256, 1024, 4096]
        ppo_arch = [64, dict(vf=[32], pi=[256, 1024, 4096])] ## NOTE: Not used
        if args.model == &#34;DQN&#34;:
            policy_kwargs = dict(
                features_extractor_class=GNN,
                features_extractor_kwargs=gnn_kwargs,
                net_arch=dqn_arch,
            )
        else:
            policy_kwargs = dict(
                features_extractor_class=GNN,
                features_extractor_kwargs=gnn_kwargs,
                net_arch=dqn_arch,
            )
            
        ## Initialize Model
        if args.model == &#34;DQN&#34;:
            assert args.save_freq &gt; args.update_freq//args.n_envs
            
            model = CustomDQN(policy=CustomDQNPolicy,
                    env=env,
                    learning_rate=args.lr,
                    buffer_size=args.buffer_size,  # 1e6
                    learning_starts=1,
                    batch_size=args.batch_size,
                    tau=1.0, # the soft update coefficient (“Polyak update”, between 0 and 1)
                    gamma=args.gamma,
                    train_freq=(args.update_freq//args.n_envs, &#34;step&#34;),
                    gradient_steps=args.opt_iter,
                    replay_buffer_class=None,
                    replay_buffer_kwargs=None,
                    optimize_memory_usage=False,
                    # target_update_interval=500,
                    exploration_fraction=0.8, # percent of learning that includes exploration
                    exploration_initial_eps=1.0, # Initial random search
                    exploration_final_eps=0.2, # final stochasticity
                    max_grad_norm=10.,
                    tensorboard_log=tb_log_dir,
                    create_eval_env=False,
                    policy_kwargs=policy_kwargs,
                    verbose=args.verbose,
                    seed=args.seed+trial,
                    device=args.cuda,
                    _init_setup_model=True)
            
        elif args.model == &#34;PPO&#34;:
            model = PPO(policy=CustomActorCriticPolicy, 
                        env=env, 
                        learning_rate=linear_schedule(args.lr), 
                        n_steps=args.update_freq//args.n_envs, 
                        batch_size=args.batch_size, 
                        n_epochs=args.opt_iter, 
                        gamma=args.gamma, 
                        gae_lambda=0.95,
                        clip_range=args.eps_clip, 
                        clip_range_vf=None, 
                        # normalize_advantage=True, 
                        ent_coef=args.ent_coef,
                        vf_coef=0.5, 
                        max_grad_norm=0.5, 
                        use_sde=False, 
                        sde_sample_freq=-1,
                        target_kl=None, 
                        tensorboard_log=tb_log_dir, 
                        create_eval_env=False, 
                        policy_kwargs=policy_kwargs,
                        verbose=args.verbose, 
                        seed=args.seed+trial, 
                        device=args.cuda, 
                        _init_setup_model=True)
            
        elif args.model == &#34;A2C&#34;:
            model = A2C(policy=CustomActorCriticPolicy,
                        env=env,
                        learning_rate=args.lr,
                        n_steps=args.update_freq//args.n_envs,
                        gamma=args.gamma,
                        gae_lambda=0.95,
                        ent_coef=0.01,
                        vf_coef=0.5,
                        max_grad_norm=0.5,
                        rms_prop_eps=1e-5,
                        use_rms_prop=True,
                        use_sde=False,
                        sde_sample_freq=-1,
                        normalize_advantage=False,
                        tensorboard_log=tb_log_dir,
                        create_eval_env=False,
                        policy_kwargs=policy_kwargs,
                        verbose=args.verbose,
                        seed=args.seed+trial,
                        device=args.cuda,
                        _init_setup_model=True)
             
        elif args.model == &#34;random&#34;:
            print(&#34;Starting random search ...&#34;)
            evaluation_rewards = []
            evaluation_designs = []
            # for _ in range(args.m_evals):
            output = []
            # for e in env:
            #     output.append(random_search(e, args.n_eval_episodes))
            with multiprocessing.Pool(max(args.n_envs, os.cpu_count()//2)) as p:
                output = p.starmap(random_search, zip(env, repeat(args.n_eval_episodes)))
            
            print(&#34;Finished random search...&#34;)
                # p.close()
            
            best_designs = []
            rewards = []
            designs = []
            lengths = []
            for o in output:
                best_designs.append(o[0])
                rewards.append(o[1])
                designs.append(o[2])
                lengths.append(o[3])
            
            # best_designs = sum(best_designs, [])
            rewards = sum(rewards, [])
            designs = sum(designs, [])
            lengths = sum(lengths, [])

            wandb.log({
                    &#39;eval/mean_episode_rew&#39;: np.mean((rewards)),
                    &#39;eval/std_episode_rew&#39;: np.std((rewards)),
                    &#39;eval/median_episode_rew&#39;: np.median((rewards)),
                    &#39;eval/mean_episode_lengths&#39;: np.mean((lengths)),
                    &#39;eval/std_episode_lengths&#39;: np.std((lengths)),
                    &#39;eval/median_episode_lengths&#39;: np.median((lengths)),})
            evaluation_rewards.append(rewards)
            evaluation_designs.append(designs)
            
            pickle.dump([evaluation_rewards, evaluation_designs], open(f&#34;evaluations/evaluation_{args.model}_{args.goal_filename}_{args.n_eval_episodes*args.n_envs}_{args.m_evals}_{run.id}.pkl&#34;, &#39;wb&#39;))
                
                
        # elif args.model == &#34;mcts&#34;:
        #     mcts_search(env, timesteps=args.steps)
            
        # elif args.model == &#34;SA&#34;:
        #     simulated_annealing(env, args.steps)
        
        
        train = not args.no_train ## TODO
        print(f&#34;Training set to: {train}&#34;)
        ## Load old checkpoint
        if args.checkpoint:
            print(&#39;Loading Checkpoint ...&#39;)
            model = model.load(args.checkpoint)
            # train = False
        
        ## Learn
        if args.model in [&#34;DQN&#34;, &#39;A2C&#39;, &#39;PPO&#39;]:    

            if train:
                print(&#34;Starting Training...&#34;)
                # Save a checkpoint 
                callback = CheckpointCallback(save_freq=args.save_freq//args.n_envs, save_path=save_dir, name_prefix=f&#39;{now}_{args.model}_model_{args.goal_filename}&#39;)
                    
                model.learn(args.steps, log_interval=5, reset_num_timesteps=False, callback=callback) 
                
                print(&#34;Finished Training...&#34;)
                 
                print(&#34;Saving Model...&#34;)
                model.save(save_dir + f&#39;{now}_{args.model}_model_{args.goal_filename}_final.zip&#39;)
            
            
            ## Evaluate Model
            evaluation_rewards = []
            evaluation_designs = []
            for i in range(args.m_evals):
                ## Initialize model seed
                model.set_random_seed(seed=i+args.seed)
                
                print(&#34;Evaluating Model...&#34;)
                rewards, lengths, designs = evaluate_policy(model, env, n_eval_episodes=args.n_eval_episodes, deterministic=False, render=False, return_episode_rewards=True) ## TODO: update
                
                wandb.log({
                    &#39;eval/mean_episode_rew&#39;: np.mean((rewards)),
                    &#39;eval/std_episode_rew&#39;: np.std((rewards)),
                    &#39;eval/median_episode_rew&#39;: np.median((rewards)),
                    &#39;eval/mean_episode_lengths&#39;: np.mean((lengths)),
                    &#39;eval/std_episode_lengths&#39;: np.std((lengths)),
                    &#39;eval/median_episode_lengths&#39;: np.median((lengths)),})
                evaluation_rewards.append(rewards)
                evaluation_designs.append(designs)
            print(&#34;Saving Evaluation Designs...&#34;)
            pickle.dump([evaluation_rewards, evaluation_designs], open(eval_dir + f&#34;{now}_{args.model}_{args.goal_filename}_{args.n_eval_episodes}_{args.m_evals}_{run.id}.pkl&#34;, &#39;wb&#39;))
                

        ## Extract Best Designs 
        if args.model in [&#34;PPO&#34;, &#34;A2C&#34;, &#34;DQN&#34;]: best_designs = env.get_attr(&#39;best_designs&#39;)

        if isinstance(best_designs, list):
        #     pdb.set_trace()
            best_dict = {}
            rewards = {}
            for o in best_designs:
                for k, v in o.items():
                    if k not in best_dict:
                        best_dict[k] = v
                        rewards[k] = v[-1]
                        continue
                    
                    if v[-1] &gt; best_dict[k][-1]:
                        best_dict[k] = v
                        rewards[k] = v[-1]

            # node_info = [list(o.values()) for o in best_designs]
            node_info = best_dict.values() # sum(node_info, [])
        elif isinstance(best_designs, dict):
            node_info = best_designs.values()
        
        ## Plot Best Designs
        for node_positions, edges, _ in node_info:
            env_kwargs[&#39;node_positions&#39;] = node_positions
            env_kwargs[&#39;edges&#39;] = edges
            tmp_env = Mech(**env_kwargs)
            tmp_env.is_terminal = True
            
            reward = tmp_env._get_reward()
            # rewards.append(reward[0])
            
            number_of_cycles = tmp_env.number_of_cycles()
            # number_of_cycles_.append(number_of_cycles)
            
            # pdb.set_trace()

            fig = tmp_env.paper_plotting()
            fig.suptitle(f&#39;Algo: {args.model} | ID: {run.id} |\n Reward: {np.round(reward[0], 3)} | Number Of Cycles: {number_of_cycles}&#39;)
            
            ## Log images
            wandb.log({&#39;best_designs&#39;: wandb.Image(fig)})
            
            plt.close(fig)
            # fig.savefig(&#39;test_fig.png&#39;)
        
        ## Log average reward
        if rewards: 
            for k, v in rewards.items():
                wandb.log({
                        &#39;Reward Best Designs&#39;: v, &#39;Number of Nodes&#39;: k})
                        # &#39;Standard Deviation Reward Best Designs&#39;: np.std(list(rewards.values())),
                        # &#39;Median Reward Best Designs&#39;: np.median(list(rewards.values())),})
        
        ## Save Designs
        if best_designs:
            
            pickle.dump(best_dict, open(uniquify(design_dir+f&#39;best_designs_{run.id}.pkl&#39;), &#39;wb&#39;))
        
        run.finish()
        # time.sleep(1)

def trace(frame, event, arg):
    # print(&#34;%s, %s:%d&#34; % (event, frame.f_code.co_filename, frame.f_lineno))
    return trace

if __name__ == &#34;__main__&#34;:
    # Initialize the parser
    parser = argparse.ArgumentParser()

    # Add parameters positional/optional 
    # default_path = &#39;data/training_seeds.pkl&#39;
    
    ## Env Args
    parser.add_argument(&#39;--max_nodes&#39;,           default=11,                      type=int,             help=&#34;number of design steps for each agent&#34;)
    parser.add_argument(&#39;--resolution&#39;,          default=11,                      type=int,             help=&#34;design resolution&#34;)
    parser.add_argument(&#39;--bound&#39;,               default=1.0,                     type=float,           help=&#34;[x,y] value range for revolute joints&#34;)
    parser.add_argument(&#39;--sample_points&#39;,       default=20,                      type=int,             help=&#34;number of sample points from FK&#34;)
    parser.add_argument(&#39;--feature_points&#39;,      default=1,                       type=int,             help=&#34;number of feature poinys for GNN&#34;)
    parser.add_argument(&#39;--goal_filename&#39;,       default=&#39;jansen_traj&#39;,           type=str,             help=&#34;Goal filename&#34;)
    parser.add_argument(&#39;--goal_path&#39;,           default=&#39;data/other_curves&#39;,     type=str,             help=&#34;path to goal file&#34;)
    parser.add_argument(&#39;--use_self_loops&#39;,      default=False,                   action=&#39;store_true&#39;,  help=&#34;Add self loops in adj matrix&#34;)
    parser.add_argument(&#39;--normalize&#39;,           default=True,                    action=&#39;store_true&#39;,  help=&#34;normalize trajectory for feature vector&#34;)
    parser.add_argument(&#39;--use_node_type&#39;,       default=False,                   action=&#39;store_true&#39;,  help=&#34;use node type id for feature vector&#34;)
    parser.add_argument(&#39;--fixed_initial_state&#39;, default=True,                    action=&#39;store_true&#39;,  help=&#34;use same initial state for all training&#34;)
    parser.add_argument(&#39;--seed&#39;,                default=123,             type=int,             help=&#34;Random seed for numpy and gym&#34;)
    parser.add_argument(&#39;--ordered&#39;,             default=True,          action=&#39;store_true&#39;,  help=&#34;Get minimum ordered distance&#34;)
    parser.add_argument(&#39;--body_constraints&#39;,    default=None,            type=float, nargs=&#39;+&#39;,  help=&#34;Non-coupler [xmin, xmax, ymin, ymax]&#34;)
    parser.add_argument(&#39;--coupler_constraints&#39;, default=None,            type=float, nargs=&#39;+&#39;,  help=&#34;coupler [xmin, xmax, ymin, ymax]&#34;)

    ## Feature Extractor Args
    parser.add_argument(&#39;--use_gnn&#39;,             default=True,          action=&#39;store_true&#39;,  help=&#34;use GNN embedding&#34;)
    parser.add_argument(&#39;--batch_normalize&#39;,     default=True,         action=&#39;store_true&#39;,  help=&#34;use batch norm&#34;)
    
    ## Model Args
    parser.add_argument(&#39;--model&#39;,               default=&#34;PPO&#34;,         type=str,             help=&#34;which model type to use Models=[DQN, A2C, PPO]&#34;)
    parser.add_argument(&#39;--n_envs&#39;,              default=1,             type=int,             help=&#34;number of parallel environments to run NOTE: only valid for A2C or PPO&#34;)
    parser.add_argument(&#39;--checkpoint&#39;,          default=None,          type=str,             help=&#39;A previous model checkpoint&#39;)
    parser.add_argument(&#39;--update_freq&#39;,         default=1000,          type=int,             help=&#34;how often to update the model via PPO&#34;)
    parser.add_argument(&#39;--opt_iter&#39;,            default=1,             type=int,             help=&#34;how many gradient steps per update&#34;)
    parser.add_argument(&#39;--eps_clip&#39;,            default=0.2,           type=float,           help=&#34;PPO epsilon clipping&#34;)
    parser.add_argument(&#39;--ent_coef&#39;,            default=0.01,           type=float,           help=&#34;PPO epsilon clipping&#34;)

    parser.add_argument(&#39;--gamma&#39;,               default=0.99,          type=float,           help=&#34;Discount factor&#34;)
    parser.add_argument(&#39;--lr&#39;,                  default=0.0001,       type=float,           help=&#34;Learning rate&#34;)
    parser.add_argument(&#39;--batch_size&#39;,          default=1000,          type=int,             help=&#34;Batch Size for Dataloader&#34;)
    parser.add_argument(&#39;--buffer_size&#39;,         default=1000000,          type=int,             help=&#34;Batch Size for Dataloader&#34;)

    ## Training Args
    parser.add_argument(&#39;--steps&#39;,               default=50000,         type=int,             help=&#39;The number of epochs to train&#39;)
    parser.add_argument(&#39;--num_trials&#39;,          default=1,             type=int,             help=&#34;How many times to run a training of the model&#34;)
    
    ## Evaluation Args
    parser.add_argument(&#39;--n_eval_episodes&#39;,     default=100,         type=int,             help=&#39;The number of epochs to train&#39;)
    parser.add_argument(&#39;--m_evals&#39;,             default=1,             type=int,             help=&#34;How many times to run a training of the model&#34;)
    
    ## Other Args
    parser.add_argument(&#39;--log_freq&#39;,         default=1000,             type=int,             help=&#34;how often to log to document&#34;)
    parser.add_argument(&#39;--save_freq&#39;,        default=10000,            type=int,             help=&#34;how often to save instances of model, buffer and render&#34;)
    parser.add_argument(&#39;--wandb_mode&#39;,       default=&#34;online&#34;,         type=str,             help=&#34;use weights and biases to log information Modes=[online, offline, disabled]&#34;)
    parser.add_argument(&#39;--wandb_project&#39;,    default=&#34;linkage_sb4&#34;,         type=str,             help=&#34;use weights and biases to log information Modes=[online, offline, disabled]&#34;)

    parser.add_argument(&#39;--verbose&#39;,          default=0,                type=int,             help=&#34;verbose from sb3&#34;)
    parser.add_argument(&#39;--cuda&#39;,             default=&#39;cpu&#39;,         type=str,             help=&#34;Which GPU to use [0, 1, 2, 3]&#34;)
    parser.add_argument(&#39;--no_train&#39;,         default=False,         action=&#39;store_true&#39;,             help=&#34;If you don&#39;t want to train&#34;)

    
    # Parse the arguments
    args = parser.parse_args()
    # pdb.set_trace()
    # Display Args
    print(args)
    
    
    # path = f&#34;./trained/{args.goal_filename}/{args.model}/06_15_2022/&#34;
    # args.checkpoint = path+os.listdir(path)[0]
    # print(&#34;CHECKPOINT LOCATION: &#34;, args.checkpoint)
    
    # sys.settrace(trace)
    main(args)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="gcp_holo.train.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@timebudget
def main(args):
    # pdb.set_trace()
    # args.body_constraints = [-3., 3., 0., 3.]
    # args.coupler_constraints = [-3., 3., -3., 0.]
    now = datetime.now().strftime(&#34;%m_%d_%Y_%HD:%M:%S&#34;)
    day = datetime.now().strftime(&#34;%m_%d_%Y&#34;) 
    
    ## Use WANDB for logging
    os.environ[&#34;WANDB_MODE&#34;] = args.wandb_mode

    ## Learn model N times
    for trial in range(args.num_trials):
        ## Initialize WANBD for Logging
        run = wandb.init(project=args.wandb_project, 
               entity=&#39;mfogelson&#39;,
               sync_tensorboard=True,
               )
        
        ## Adds all of the arguments as config variables
        wandb.config.update(args) 

        ## Log / eval / save location
        tb_log_dir = f&#34;./logs/{args.goal_filename}/{args.model}/{day}/{run.id}&#34;
        eval_dir = f&#34;./evaluations/{args.goal_filename}/{args.model}/{day}/&#34;
        save_dir = f&#34;./trained/{args.goal_filename}/{args.model}/{day}/&#34;
        design_dir = f&#34;./designs/{args.goal_filename}/{args.model}/{day}/&#34;
        
        if not os.path.isdir(tb_log_dir):
            os.makedirs(tb_log_dir, exist_ok=True)
            
        if not os.path.isdir(eval_dir):
            os.makedirs(eval_dir, exist_ok=True)
            
        if not os.path.isdir(save_dir):
            os.makedirs(save_dir, exist_ok=True)
            
        if not os.path.isdir(design_dir):
            os.makedirs(design_dir, exist_ok=True)

        ## Load Goal information
        goal_curve = pickle.load(open(f&#39;{args.goal_path}/{args.goal_filename}.pkl&#39;, &#39;rb&#39;)) # NOTE: sometimes ordering needs to be reversed add [:,::-1]
            
        idx = np.round(np.linspace(0, goal_curve.shape[1] - 1, args.sample_points)).astype(int)
        goal = normalize_curve(goal_curve[:,idx]) #R@normalize_curve(goal_curve[:,::-1][:,idx])
        goal[:, -1] = goal[:, 0]
        
        ## Initialize Gym ENV
        env_kwargs = {&#34;max_nodes&#34;:args.max_nodes, 
                        &#34;bound&#34;:args.bound, 
                        &#34;resolution&#34;:args.resolution, 
                        &#34;sample_points&#34;:args.sample_points,
                        &#34;feature_points&#34;: args.feature_points, 
                        &#34;goal&#34;:goal, 
                        &#34;normalize&#34;:args.normalize, 
                        # &#34;seed&#34;: args.seed+trial, 
                        &#34;fixed_initial_state&#34;: args.fixed_initial_state, 
                        &#34;ordered_distance&#34;: args.ordered, 
                        &#34;constraints&#34;: [], #[args.body_constraints, args.coupler_constraints], 
                        &#34;self_loops&#34;: args.use_self_loops, 
                        &#34;use_node_type&#34;: args.use_node_type,}
        
        # If PPO A2C or DQN can use multiple envs while training
        if args.model in [&#39;PPO&#39;, &#39;A2C&#39;, &#39;DQN&#39;]:
            env = make_vec_env(Mech, n_envs=args.n_envs, env_kwargs=env_kwargs, seed=args.seed) # NOTE: For faster training use SubProcVecEnv 
        else:
            env = []
            for i in range(args.n_envs):
                env_kwargs[&#39;seed&#39;] = i+args.seed
                env.append(Mech(**env_kwargs))
                        
        ## GNN Args
        gnn_kwargs = {
                &#34;max_nodes&#34;: args.max_nodes,
                &#34;num_features&#34;: 2*args.feature_points+int(args.use_node_type),
                &#34;hidden_channels&#34;:64, 
                &#34;out_channels&#34;:64, 
                &#34;normalize&#34;:False, 
                &#34;batch_normalization&#34;:args.batch_normalize, 
                &#34;lin&#34;:True, 
                &#34;add_loop&#34;:False}
        
        ## Policy Architecture
        dqn_arch = [64, 256, 1024, 4096]
        ppo_arch = [64, dict(vf=[32], pi=[256, 1024, 4096])] ## NOTE: Not used
        if args.model == &#34;DQN&#34;:
            policy_kwargs = dict(
                features_extractor_class=GNN,
                features_extractor_kwargs=gnn_kwargs,
                net_arch=dqn_arch,
            )
        else:
            policy_kwargs = dict(
                features_extractor_class=GNN,
                features_extractor_kwargs=gnn_kwargs,
                net_arch=dqn_arch,
            )
            
        ## Initialize Model
        if args.model == &#34;DQN&#34;:
            assert args.save_freq &gt; args.update_freq//args.n_envs
            
            model = CustomDQN(policy=CustomDQNPolicy,
                    env=env,
                    learning_rate=args.lr,
                    buffer_size=args.buffer_size,  # 1e6
                    learning_starts=1,
                    batch_size=args.batch_size,
                    tau=1.0, # the soft update coefficient (“Polyak update”, between 0 and 1)
                    gamma=args.gamma,
                    train_freq=(args.update_freq//args.n_envs, &#34;step&#34;),
                    gradient_steps=args.opt_iter,
                    replay_buffer_class=None,
                    replay_buffer_kwargs=None,
                    optimize_memory_usage=False,
                    # target_update_interval=500,
                    exploration_fraction=0.8, # percent of learning that includes exploration
                    exploration_initial_eps=1.0, # Initial random search
                    exploration_final_eps=0.2, # final stochasticity
                    max_grad_norm=10.,
                    tensorboard_log=tb_log_dir,
                    create_eval_env=False,
                    policy_kwargs=policy_kwargs,
                    verbose=args.verbose,
                    seed=args.seed+trial,
                    device=args.cuda,
                    _init_setup_model=True)
            
        elif args.model == &#34;PPO&#34;:
            model = PPO(policy=CustomActorCriticPolicy, 
                        env=env, 
                        learning_rate=linear_schedule(args.lr), 
                        n_steps=args.update_freq//args.n_envs, 
                        batch_size=args.batch_size, 
                        n_epochs=args.opt_iter, 
                        gamma=args.gamma, 
                        gae_lambda=0.95,
                        clip_range=args.eps_clip, 
                        clip_range_vf=None, 
                        # normalize_advantage=True, 
                        ent_coef=args.ent_coef,
                        vf_coef=0.5, 
                        max_grad_norm=0.5, 
                        use_sde=False, 
                        sde_sample_freq=-1,
                        target_kl=None, 
                        tensorboard_log=tb_log_dir, 
                        create_eval_env=False, 
                        policy_kwargs=policy_kwargs,
                        verbose=args.verbose, 
                        seed=args.seed+trial, 
                        device=args.cuda, 
                        _init_setup_model=True)
            
        elif args.model == &#34;A2C&#34;:
            model = A2C(policy=CustomActorCriticPolicy,
                        env=env,
                        learning_rate=args.lr,
                        n_steps=args.update_freq//args.n_envs,
                        gamma=args.gamma,
                        gae_lambda=0.95,
                        ent_coef=0.01,
                        vf_coef=0.5,
                        max_grad_norm=0.5,
                        rms_prop_eps=1e-5,
                        use_rms_prop=True,
                        use_sde=False,
                        sde_sample_freq=-1,
                        normalize_advantage=False,
                        tensorboard_log=tb_log_dir,
                        create_eval_env=False,
                        policy_kwargs=policy_kwargs,
                        verbose=args.verbose,
                        seed=args.seed+trial,
                        device=args.cuda,
                        _init_setup_model=True)
             
        elif args.model == &#34;random&#34;:
            print(&#34;Starting random search ...&#34;)
            evaluation_rewards = []
            evaluation_designs = []
            # for _ in range(args.m_evals):
            output = []
            # for e in env:
            #     output.append(random_search(e, args.n_eval_episodes))
            with multiprocessing.Pool(max(args.n_envs, os.cpu_count()//2)) as p:
                output = p.starmap(random_search, zip(env, repeat(args.n_eval_episodes)))
            
            print(&#34;Finished random search...&#34;)
                # p.close()
            
            best_designs = []
            rewards = []
            designs = []
            lengths = []
            for o in output:
                best_designs.append(o[0])
                rewards.append(o[1])
                designs.append(o[2])
                lengths.append(o[3])
            
            # best_designs = sum(best_designs, [])
            rewards = sum(rewards, [])
            designs = sum(designs, [])
            lengths = sum(lengths, [])

            wandb.log({
                    &#39;eval/mean_episode_rew&#39;: np.mean((rewards)),
                    &#39;eval/std_episode_rew&#39;: np.std((rewards)),
                    &#39;eval/median_episode_rew&#39;: np.median((rewards)),
                    &#39;eval/mean_episode_lengths&#39;: np.mean((lengths)),
                    &#39;eval/std_episode_lengths&#39;: np.std((lengths)),
                    &#39;eval/median_episode_lengths&#39;: np.median((lengths)),})
            evaluation_rewards.append(rewards)
            evaluation_designs.append(designs)
            
            pickle.dump([evaluation_rewards, evaluation_designs], open(f&#34;evaluations/evaluation_{args.model}_{args.goal_filename}_{args.n_eval_episodes*args.n_envs}_{args.m_evals}_{run.id}.pkl&#34;, &#39;wb&#39;))
                
                
        # elif args.model == &#34;mcts&#34;:
        #     mcts_search(env, timesteps=args.steps)
            
        # elif args.model == &#34;SA&#34;:
        #     simulated_annealing(env, args.steps)
        
        
        train = not args.no_train ## TODO
        print(f&#34;Training set to: {train}&#34;)
        ## Load old checkpoint
        if args.checkpoint:
            print(&#39;Loading Checkpoint ...&#39;)
            model = model.load(args.checkpoint)
            # train = False
        
        ## Learn
        if args.model in [&#34;DQN&#34;, &#39;A2C&#39;, &#39;PPO&#39;]:    

            if train:
                print(&#34;Starting Training...&#34;)
                # Save a checkpoint 
                callback = CheckpointCallback(save_freq=args.save_freq//args.n_envs, save_path=save_dir, name_prefix=f&#39;{now}_{args.model}_model_{args.goal_filename}&#39;)
                    
                model.learn(args.steps, log_interval=5, reset_num_timesteps=False, callback=callback) 
                
                print(&#34;Finished Training...&#34;)
                 
                print(&#34;Saving Model...&#34;)
                model.save(save_dir + f&#39;{now}_{args.model}_model_{args.goal_filename}_final.zip&#39;)
            
            
            ## Evaluate Model
            evaluation_rewards = []
            evaluation_designs = []
            for i in range(args.m_evals):
                ## Initialize model seed
                model.set_random_seed(seed=i+args.seed)
                
                print(&#34;Evaluating Model...&#34;)
                rewards, lengths, designs = evaluate_policy(model, env, n_eval_episodes=args.n_eval_episodes, deterministic=False, render=False, return_episode_rewards=True) ## TODO: update
                
                wandb.log({
                    &#39;eval/mean_episode_rew&#39;: np.mean((rewards)),
                    &#39;eval/std_episode_rew&#39;: np.std((rewards)),
                    &#39;eval/median_episode_rew&#39;: np.median((rewards)),
                    &#39;eval/mean_episode_lengths&#39;: np.mean((lengths)),
                    &#39;eval/std_episode_lengths&#39;: np.std((lengths)),
                    &#39;eval/median_episode_lengths&#39;: np.median((lengths)),})
                evaluation_rewards.append(rewards)
                evaluation_designs.append(designs)
            print(&#34;Saving Evaluation Designs...&#34;)
            pickle.dump([evaluation_rewards, evaluation_designs], open(eval_dir + f&#34;{now}_{args.model}_{args.goal_filename}_{args.n_eval_episodes}_{args.m_evals}_{run.id}.pkl&#34;, &#39;wb&#39;))
                

        ## Extract Best Designs 
        if args.model in [&#34;PPO&#34;, &#34;A2C&#34;, &#34;DQN&#34;]: best_designs = env.get_attr(&#39;best_designs&#39;)

        if isinstance(best_designs, list):
        #     pdb.set_trace()
            best_dict = {}
            rewards = {}
            for o in best_designs:
                for k, v in o.items():
                    if k not in best_dict:
                        best_dict[k] = v
                        rewards[k] = v[-1]
                        continue
                    
                    if v[-1] &gt; best_dict[k][-1]:
                        best_dict[k] = v
                        rewards[k] = v[-1]

            # node_info = [list(o.values()) for o in best_designs]
            node_info = best_dict.values() # sum(node_info, [])
        elif isinstance(best_designs, dict):
            node_info = best_designs.values()
        
        ## Plot Best Designs
        for node_positions, edges, _ in node_info:
            env_kwargs[&#39;node_positions&#39;] = node_positions
            env_kwargs[&#39;edges&#39;] = edges
            tmp_env = Mech(**env_kwargs)
            tmp_env.is_terminal = True
            
            reward = tmp_env._get_reward()
            # rewards.append(reward[0])
            
            number_of_cycles = tmp_env.number_of_cycles()
            # number_of_cycles_.append(number_of_cycles)
            
            # pdb.set_trace()

            fig = tmp_env.paper_plotting()
            fig.suptitle(f&#39;Algo: {args.model} | ID: {run.id} |\n Reward: {np.round(reward[0], 3)} | Number Of Cycles: {number_of_cycles}&#39;)
            
            ## Log images
            wandb.log({&#39;best_designs&#39;: wandb.Image(fig)})
            
            plt.close(fig)
            # fig.savefig(&#39;test_fig.png&#39;)
        
        ## Log average reward
        if rewards: 
            for k, v in rewards.items():
                wandb.log({
                        &#39;Reward Best Designs&#39;: v, &#39;Number of Nodes&#39;: k})
                        # &#39;Standard Deviation Reward Best Designs&#39;: np.std(list(rewards.values())),
                        # &#39;Median Reward Best Designs&#39;: np.median(list(rewards.values())),})
        
        ## Save Designs
        if best_designs:
            
            pickle.dump(best_dict, open(uniquify(design_dir+f&#39;best_designs_{run.id}.pkl&#39;), &#39;wb&#39;))
        
        run.finish()
        # time.sleep(1)</code></pre>
</details>
</dd>
<dt id="gcp_holo.train.trace"><code class="name flex">
<span>def <span class="ident">trace</span></span>(<span>frame, event, arg)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def trace(frame, event, arg):
    # print(&#34;%s, %s:%d&#34; % (event, frame.f_code.co_filename, frame.f_lineno))
    return trace</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gcp_holo" href="index.html">gcp_holo</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="gcp_holo.train.main" href="#gcp_holo.train.main">main</a></code></li>
<li><code><a title="gcp_holo.train.trace" href="#gcp_holo.train.trace">trace</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>